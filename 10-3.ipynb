{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lp7k8Ul6VWg"
      },
      "source": [
        "# ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ë¡œ í…ìŠ¤íŠ¸ ìƒì„±í•˜ê¸°"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekJOhc986a7p"
      },
      "source": [
        "<table align=\"left\"><tr><td>\n",
        "<a href=\"https://colab.research.google.com/github/rickiepark/hg-mldl2/blob/main/10-3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"ì½”ë©ì—ì„œ ì‹¤í–‰í•˜ê¸°\"/></a>\n",
        "</td></tr></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXAONE 3.5 ëª¨ë¸ì€ `transformers` ìµœì‹  ë²„ì „ê³¼ í˜¸í™˜ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì´ ì ˆì˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ê¸° ìœ„í•´ `transformers` 5.1.0 ë²„ì „ì„ ì„¤ì¹˜í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "ZoFfCoeq4ZFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers==5.1.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pU9bLPP7LrC",
        "outputId": "3afa1502-4c85-4dea-bbaa-d4c5c98a7c5c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==5.1.0\n",
            "  Downloading transformers-5.1.0-py3-none-any.whl.metadata (31 kB)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from transformers==5.1.0) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==5.1.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==5.1.0) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==5.1.0) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==5.1.0) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers==5.1.0) (0.22.2)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers==5.1.0) (0.24.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers==5.1.0) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==5.1.0) (4.67.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers==5.1.0) (3.24.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers==5.1.0) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers==5.1.0) (1.3.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers==5.1.0) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers==5.1.0) (1.5.4)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers==5.1.0) (4.15.0)\n",
            "Requirement already satisfied: typer>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers==5.1.0) (0.24.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers==5.1.0) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers==5.1.0) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers==5.1.0) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers==5.1.0) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers==5.1.0) (0.16.0)\n",
            "Requirement already satisfied: click>=8.2.1 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->transformers==5.1.0) (8.3.1)\n",
            "Requirement already satisfied: rich>=12.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->transformers==5.1.0) (13.9.4)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->transformers==5.1.0) (0.0.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->transformers==5.1.0) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->transformers==5.1.0) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=12.3.0->typer>=0.24.0->typer-slim->transformers==5.1.0) (0.1.2)\n",
            "Downloading transformers-5.1.0-py3-none-any.whl (10.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 5.0.0\n",
            "    Uninstalling transformers-5.0.0:\n",
            "      Successfully uninstalled transformers-5.0.0\n",
            "Successfully installed transformers-5.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ê¹ƒí—ˆë¸Œì—ì„œ ìœ„ì ¯ ìƒíƒœ ì˜¤ë¥˜ë¥¼ í”¼í•˜ê¸° ìœ„í•´ ì§„í–‰ í‘œì‹œì¤„ì„ ë‚˜íƒ€ë‚´ì§€ ì•Šë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.\n",
        "from transformers.utils import logging\n",
        "\n",
        "logging.disable_progress_bar()"
      ],
      "metadata": {
        "id": "1Q2E2reQUuWB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMzjB7rbojXF"
      },
      "source": [
        "## EXAONE-3.5ë¡œ ìƒí’ˆ ì§ˆë¬¸ì— ëŒ€í•œ ëŒ€ë‹µ ìƒì„±í•˜ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "orlM-1oS35V0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b09aefa6-5e67-4afd-c9d2-ece8785de29e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct:\n",
            "- configuration_exaone.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "exaone_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\", trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vttIh-QO-1jh",
        "outputId": "dd1d77f9-ac50-4cbf-bc00-56f2063fb4a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct:\n",
            "- modeling_exaone.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(task=\"text-generation\",\n",
        "                model=\"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\",\n",
        "                tokenizer=exaone_tokenizer,\n",
        "                device=0, trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suKA-HzT4BT3",
        "outputId": "841d61b1-b2cd-447f-b429-10c52189db38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Passing `generation_config` together with generation-related arguments=({'max_new_tokens'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
            "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': [{'role': 'system',\n",
              "    'content': 'ë„ˆëŠ” ì‡¼í•‘ëª° í™ˆí˜ì´ì§€ì— ì˜¬ë¼ì˜¨ ì§ˆë¬¸ì— ëŒ€ë‹µí•˜ëŠ” Q&A ì±—ë´‡ì´ì•¼.                  í™•ì •ì ì¸ ë‹µë³€ì„ í•˜ì§€ ë§ê³  ì œí’ˆ ë‹´ë‹¹ìê°€ ì •í™•í•œ ë‹µë³€ì„ í•˜ê¸° ìœ„í•´                  ì‹œê°„ì´ í•„ìš”í•˜ë‹¤ëŠ” ê°„ë‹¨í•˜ê³  ì¹œì ˆí•œ ë‹µë³€ì„ ìƒì„±í•´ì¤˜.'},\n",
              "   {'role': 'user', 'content': 'ì´ ë‹¤ì´ì–´ë¦¬ì— ë‚´ë…„ë„ ê³µíœ´ì¼ì´ í‘œì‹œë˜ì–´ ìˆë‚˜ìš”?'},\n",
              "   {'role': 'assistant',\n",
              "    'content': 'ì•ˆë…•í•˜ì„¸ìš”! ë‹¤ì´ì–´ë¦¬ì— ë‚´ë…„ì˜ ê³µíœ´ì¼ì´ ë¯¸ë¦¬ í‘œì‹œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ëŠ” ê±´ ì •ë§ ì¢‹ì€ ì§ˆë¬¸ì´ë„¤ìš”. ì •í™•í•œ ë‹µë³€ì„ ë“œë¦¬ë ¤ë©´ ì œí’ˆ ë‹´ë‹¹ìë‹˜ê»˜ ë¬¸ì˜í•˜ì‹œëŠ” ê²ƒì´ ê°€ì¥ ë¹ ë¥¼ ê²ƒ ê°™ì•„ìš”. ë‹´ë‹¹ìë‹˜ê»˜ì„œëŠ” ë‹¤ì´ì–´ë¦¬ì˜ ì •í™•í•œ ì‚¬ì–‘ê³¼ ì—…ë°ì´íŠ¸ ì‚¬í•­ì— ëŒ€í•´ ìì„¸íˆ ì•Œê³  ê³„ì‹¤ í…Œë‹ˆê¹Œìš”. ë‹´ë‹¹ìë‹˜ê»˜ ì—°ë½í•˜ì‹œë©´ ë°”ë¡œ í™•ì¸í•´ ì£¼ì‹¤ ê±°ì˜ˆìš”. ë„ì™€ë“œë¦¬ê¸° ì–´ë ¤ì›Œì„œ ì£„ì†¡í•©ë‹ˆë‹¤! ğŸ˜Š'}]}]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\",\n",
        "     \"content\": \"ë„ˆëŠ” ì‡¼í•‘ëª° í™ˆí˜ì´ì§€ì— ì˜¬ë¼ì˜¨ ì§ˆë¬¸ì— ëŒ€ë‹µí•˜ëŠ” Q&A ì±—ë´‡ì´ì•¼. \\\n",
        "                 í™•ì •ì ì¸ ë‹µë³€ì„ í•˜ì§€ ë§ê³  ì œí’ˆ ë‹´ë‹¹ìê°€ ì •í™•í•œ ë‹µë³€ì„ í•˜ê¸° ìœ„í•´ \\\n",
        "                 ì‹œê°„ì´ í•„ìš”í•˜ë‹¤ëŠ” ê°„ë‹¨í•˜ê³  ì¹œì ˆí•œ ë‹µë³€ì„ ìƒì„±í•´ì¤˜.\"},\n",
        "    {\"role\": \"user\", \"content\": \"ì´ ë‹¤ì´ì–´ë¦¬ì— ë‚´ë…„ë„ ê³µíœ´ì¼ì´ í‘œì‹œë˜ì–´ ìˆë‚˜ìš”?\"}\n",
        "]\n",
        "\n",
        "pipe(messages, max_new_tokens=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6D0ObeXeM8f",
        "outputId": "91c748b3-2f53-4110-99f0-51889bf19c0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'ë„¤, ë§ìŠµë‹ˆë‹¤! ë‹¤ì´ì–´ë¦¬ì— ë‚´ë…„ë„ ê³µíœ´ì¼ì´ í‘œì‹œë˜ì–´ ìˆì„ ê±°ì˜ˆìš”. í•˜ì§€ë§Œ ì •í™•í•œ ê³µíœ´ì¼ ì •ë³´ì™€ ìœ„ì¹˜ë¥¼ í™•ì¸í•˜ë ¤ë©´ ì œí’ˆ ë‹´ë‹¹ìë‹˜ê»˜ ë¬¸ì˜í•˜ì‹œëŠ” ê²Œ ê°€ì¥ í™•ì‹¤í•  ê²ƒ ê°™ìŠµë‹ˆë‹¤. ë‹´ë‹¹ìë¶„ê»˜ ì§ì ‘ ë§ì”€ë“œë¦¬ì‹œê±°ë‚˜, ê³ ê° ì„œë¹„ìŠ¤ íŒ€ì— ì—°ë½í•˜ì‹œë©´ ë°”ë¡œ ë‹µë³€ì„ ë°›ìœ¼ì‹¤ ìˆ˜ ìˆì„ ê²ƒ ê°™ì•„ìš”. ê°ì‚¬í•©ë‹ˆë‹¤!'}]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "pipe(messages, max_new_tokens=200, return_full_text=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rX3PhJ291jw0",
        "outputId": "8edb96a1-5662-46f0-ab1b-6edf9b1c3eac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Passing `generation_config` together with generation-related arguments=({'max_new_tokens', 'do_sample'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
            "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì•ˆë…•í•˜ì„¸ìš”! í•´ë‹¹ ë‹¤ì´ì–´ë¦¬ì˜ ë‚´ë…„ ê³µíœ´ì¼ ì •ë³´ë¥¼ í™•ì¸í•´ë“œë¦¬ê¸° ìœ„í•´ ì œí’ˆ ë‹´ë‹¹ìë‹˜ê»˜ ë¬¸ì˜í•˜ì‹  ê²ƒ ê°™ë„¤ìš”. ì§€ê¸ˆ ë°”ë¡œ ë‹µë³€ë“œë¦¬ê¸°ë³´ë‹¤ëŠ” ì¡°ê¸ˆë§Œ ê¸°ë‹¤ë ¤ ì£¼ì‹œë©´ ë©ë‹ˆë‹¤. ë‹´ë‹¹ìë¶„ê»˜ì„œ ìµœì‹  ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€í•´ ì£¼ì‹¤ ê±°ì˜ˆìš”. ë‹¤ì‹œ í•œë²ˆ ë¬¼ì–´ë³´ì‹œë©´ ë” ë¹¨ë¦¬ í™•ì¸í•´ë“œë¦´ ìˆ˜ ìˆìœ¼ë‹ˆ ì°¸ê³  ë¶€íƒë“œë¦½ë‹ˆë‹¤! ê°ì‚¬í•©ë‹ˆë‹¤.\n"
          ]
        }
      ],
      "source": [
        "output = pipe(messages, max_new_tokens=200, return_full_text=False,\n",
        "              do_sample=True)\n",
        "print(output[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pU1i5vWKodxq"
      },
      "source": [
        "## í† í° ë””ì½”ë”© ì „ëµ\n",
        "\n",
        "### ê¸°ë³¸ ìƒ˜í”Œë§"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Eo_cR5cRzZwr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "logits = np.array([1, 2, 3, 4, 100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czQNsdyHyQG4",
        "outputId": "6892e18a-66d4-43b2-cb60-9c4531010eff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.01122149e-43 2.74878501e-43 7.47197234e-43 2.03109266e-42\n",
            " 1.00000000e+00]\n"
          ]
        }
      ],
      "source": [
        "from scipy.special import softmax\n",
        "\n",
        "probas = softmax(logits)\n",
        "print(probas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzV5-Erxzb3T",
        "outputId": "02807d04-ff34-4e39-ed4a-cdbb22b259c3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  0,   0,   0,   0, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "np.random.multinomial(100, probas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1hiMBbP2fo-",
        "outputId": "3b9eb2c1-8296-4593-fc6a-967892ad9d86"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([12, 20, 11, 14, 43])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "probas = softmax(logits/100)\n",
        "np.random.multinomial(100, probas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVL3CbAW2ki5",
        "outputId": "fd079217-82d4-4333-ba91-1a54c25544f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Passing `generation_config` together with generation-related arguments=({'do_sample', 'max_new_tokens', 'temperature'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
            "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì €í¬ ê´€ë¦¬ìí•œë° í•´ë‹¹ ë‹¤ì´ì–´ë¦¬ í˜ì´ì§€ë³„ ì„¤ì • ì—¬ë¶€, ì •í™•í•˜ê¸°ëŠ”ì§€ ì•Œì•„ë³´ì•„ì£¼ì‹œë„ë¡ íŠ¹ë³„ ì§€ì¹¨ ìš”ì²­ì€ ì–´ë– ì„¸ìš”ğŸ§“ã€‚ ê·¸ëŸ¼ ì €í¬ë¥¼ ì ì‹œë§Œ ë– ë‚˜ ê´€ë¦¬ìë“¤ì´ ë‹µ ë§ì¶° ë“œë¦´ê²Œìš” ì •ë§ ì£„ì†¡ìŠ¤ë ˆ ì§€ì—° ì‚¬ìœ ì§€ë§Œ ìµœì„  ê°€ëŠ¥í•©ë‹ˆë‹¤ ğŸ“¡  í•„ìš”í•˜ê¸°ì‹  ì§ˆë¬¸ì´ë‹ˆê¹Œìš” ì°¸ê³ ìš©ë£Œ ì²œì²œíˆ ì•Œì•„ë´ ì¤„ê»Ÿë£Œ ë¶€íƒë“œë ¤ìš” thankã€”yourteam__|youur_íŒ€] ë‹˜ í™•ì¸í•´ë³´ê³  ì‘ë‹µ ê¸°ë‹¤ë¦¬ì…¨êµ°ìš”ì—! ğŸ¤”âœ¨ ì¶”ê°€ì •ë³´ ì œê³µ ì˜ˆì • ì¤‘ì´ì˜µì‡¼ ğŸ“„_cal _ _ap _pear.â€ í™•ì¸ì„ ê¸°ë‹¤ë ¤ì•¼í•˜ì§€ë§Œ ê°ì‚¬í•˜ê³  ì‘ì› í• ê²Œì˜¤ì•„ : )\n",
            "\n",
            "ìì„¸ìš´ë‹µë³€ê¸°ëŒ€í•©ë‹ˆë‹¤!ğŸ¦‹â¤â¤   ì°¸ê³ ë¡œ ì¡°ê¸ˆ Delayë¡œ ì§„í–‰ ì˜ˆì • ì„ì˜¤ë‚˜ ì¤‘ìš”í•¨ì´ì´ë‹ˆ ì ê·¹ ê¸°ë‹¤ë ¤ì•¼ í•œë‹¤ê³  ì „í•©ë‹ˆë‹¤ ê°ì‚¬í•˜ì‹­ì‹œì˜¤!! ê¸°ëŒ€ ë˜ë„¤ìš”. ğŸ¤ ğŸ’soon ğŸ¤• ğŸ¦³soon... ì ì‹œë§Œ ê²¬ëŒì¤˜ì„œê³ ë§™êµ°ìš” âœ¨. ê³„ì† ì—°ë½í•´ìš”, ë„ì™€ì•¼ í•¨ ì—°ë½ ì˜ˆì •.\n"
          ]
        }
      ],
      "source": [
        "output = pipe(messages, max_new_tokens=200, return_full_text=False,\n",
        "              do_sample=True, temperature=10.0)\n",
        "print(output[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOp0DaJq5lTc",
        "outputId": "63cbce95-da14-45cf-9ea4-e643af8a01ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì•ˆë…•í•˜ì„¸ìš”! ë‹¤ì´ì–´ë¦¬ì— ë‚´ë…„ì˜ ê³µíœ´ì¼ì´ ë¯¸ë¦¬ í‘œì‹œë˜ì–´ ìˆëŠ”ì§€ì— ëŒ€í•´ ì •í™•í•œ ë‹µë³€ì„ ë“œë¦¬ê¸° ìœ„í•´ì„œëŠ” ì œí’ˆ ë‹´ë‹¹ìì—ê²Œ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤. í˜„ì¬ë¡œì„  ì§ì ‘ í™•ì¸ì´ ì–´ë ¤ìš°ë‹ˆ, ì €í¬ê°€ ì•ˆë‚´ë“œë¦´ ìˆ˜ ìˆëŠ” ë°©ë²•ìœ¼ë¡œëŠ” ê³ ê°ì„¼í„°ì— ì—°ë½í•˜ì‹œê±°ë‚˜, ì œí’ˆ í˜ì´ì§€ ë‚´ì˜ ë¬¸ì˜ ê²Œì‹œíŒì„ í†µí•´ ì§ˆë¬¸í•´ ë³´ì‹œëŠ” ê²ƒì´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ë‹´ë‹¹ìë¶„ê»˜ì„œ ë¹ ë¥´ê²Œ ë‹µë³€í•´ ì£¼ì‹¤ ê±°ì˜ˆìš”! ê°ì‚¬í•©ë‹ˆë‹¤.\n"
          ]
        }
      ],
      "source": [
        "output = pipe(messages, max_new_tokens=200, return_full_text=False,\n",
        "              do_sample=True, temperature=0.001)\n",
        "print(output[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRH9m1tAox0c"
      },
      "source": [
        "### top-k ìƒ˜í”Œë§"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0U02SYXo0tA",
        "outputId": "27adc7ac-55e2-47f1-c42c-e9555680ea86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Passing `generation_config` together with generation-related arguments=({'top_k', 'do_sample', 'max_new_tokens'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
            "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì•ˆë…•í•˜ì„¸ìš”! ë‹¤ì´ì–´ë¦¬ì— ë‚´ë…„ì˜ ê³µíœ´ì¼ì´ ë¯¸ë¦¬ í‘œì‹œë˜ì–´ ìˆëŠ”ì§€ì— ëŒ€í•œ ë‹µë³€ì„ ë“œë¦¬ë ¤ë©´, ì œí’ˆ ë‹´ë‹¹ìë‹˜ê»˜ì„œ í•´ë‹¹ ë‹¤ì´ì–´ë¦¬ì˜ ì œì‘ ì—°ë„ì™€ ìµœì‹  ì—…ë°ì´íŠ¸ ë‚´ìš©ì„ í™•ì¸í•˜ì‹¤ í•„ìš”ê°€ ìˆì–´ìš”. ì •í™•í•œ ì •ë³´ë¥¼ ì–»ìœ¼ì‹œë ¤ë©´ ì €í¬ì—ê²Œ ë°”ë¡œ ë¬¸ì˜í•´ ì£¼ì‹œê±°ë‚˜ ê³ ê° ì„œë¹„ìŠ¤ íŒ€ì— ì—°ë½í•˜ì‹œëŠ” ê²ƒì„ ì¶”ì²œë“œë¦½ë‹ˆë‹¤. ê³§ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆë„ë¡ ë…¸ë ¥í•˜ê² ìŠµë‹ˆë‹¤!\n"
          ]
        }
      ],
      "source": [
        "output = pipe(messages, max_new_tokens=200, return_full_text=False,\n",
        "              do_sample=True, top_k=10)\n",
        "print(output[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sd8Bumm5qcKs",
        "outputId": "f3ed182f-7c84-4620-9f3a-45a78c68dc5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Passing `generation_config` together with generation-related arguments=({'top_k', 'do_sample', 'max_new_tokens', 'temperature'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
            "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ê°ì‚¬ í•©ë‹ˆë‹¤! ì§ˆë¬¸ ê°ì‚¬ë“œë ¤ìš” í•˜ì§€ë§Œ í˜„ì¬ ì œê³µí•˜ë ¤ í•˜ëŠ”ë°ëŠ” ì‹œê°„ì ì¸ì œí•œì‚¬í•­ì´ê°€ì˜¤ë‹ˆ ì •í™•íˆ ë§ì”€í•´ ë“œë¦´ê²Œìš”â€”ë‚´ë…„ì— ëŒ€í•œ êµ¬ì²´ì‚¬í•­ì— ëŒ€í•´ì„œëŠ” ì œí’ˆì— ë°”ë¡œ ì ìš©í•˜ë ¤ë©´ í•´ë‹¹ ì‹œì ê¹Œì§€ ì •ë³´ ìˆ˜ì •ì„ ì™„ë£Œí•˜ê³  ë‚˜ì„œ ë‚˜ì˜¬ ë¶€ë¶„ì´ê¸°ë¡œ ë˜ì–´ìˆë„¤ìš” ğŸ˜Š ì œí’ˆ ì¶œì‹œ ì‹œê¸° í˜¹ì€ í–¥í›„ ì •ë³´ë¥¼ ì œê³µí•˜ë ¤ë©´ ì €í¬ ì‹œê°„ì´ ë§ì¶°ì§ˆ ì˜ˆì •ì…ë‹ˆë‹¤. í˜¹ì‹œ ë” ì¶”ê°€ ì§ˆë¬¸í•´ ì£¼ì‹œì§€ë“  ë°”ë¡œ ë„ì™€ë“œë¦¬ê±°ë‚˜ ê´€ë ¨ ì •ë³´ë¥¼ ì•ˆë‚´ë°›ìœ¼ì‹œë ¤ë©´ ê¸°ë‹¤ë¦¬ì…”ë„ ë  ë“¯ í•©ë‹ˆë‹¤~ ì–¸ì œë“  ë‹¤ì‹œ ë¬¼ì–´ë´ ì£¼ì‹ ë‹¤ë‹ˆ ê¸°ë‹¤ë¦¬ì„¸ìš”!\n"
          ]
        }
      ],
      "source": [
        "output = pipe(messages, max_new_tokens=200, return_full_text=False,\n",
        "              do_sample=True, top_k=10, temperature=10.0)\n",
        "print(output[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEfCc4wCrcXd"
      },
      "source": [
        "### top-p ìƒ˜í”Œë§"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7sZieDNreBn",
        "outputId": "8b37edb6-01e9-4e69-f7e2-471c62dd10b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Passing `generation_config` together with generation-related arguments=({'top_p', 'do_sample', 'max_new_tokens'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
            "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì•ˆë…•í•˜ì„¸ìš”! ë‹¤ì´ì–´ë¦¬ì— ë‚´ë…„ì˜ ê³µíœ´ì¼ ì •ë³´ê°€ ë¯¸ë¦¬ í‘œì‹œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ë“œë¦¬ë ¤ë©´ ì œí’ˆ ë‹´ë‹¹ìê°€ ì§ì ‘ ê²€í† í•´ì•¼ í•©ë‹ˆë‹¤. í˜„ì¬ë¡œì„  ì •í™•í•œ ë‹µë³€ì„ ë“œë¦¬ê¸° ì–´ë µì§€ë§Œ, ì œí’ˆ ì¶œì‹œ ì‹œ ì œê³µë˜ëŠ” ì •ë³´ë‚˜ ê³ ê° ì„œë¹„ìŠ¤ íŒ€ì— ë¬¸ì˜í•˜ì‹œë©´ ë” ë¹ ë¥´ê²Œ ë‹µë³€ì„ ë°›ìœ¼ì‹¤ ìˆ˜ ìˆì„ ê±°ì˜ˆìš”. ë„ì›€ì´ ë˜ì…¨ê¸¸ ë°”ëë‹ˆë‹¤!\n"
          ]
        }
      ],
      "source": [
        "output = pipe(messages, max_new_tokens=200, return_full_text=False,\n",
        "              do_sample=True, top_p=0.9)\n",
        "print(output[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QN0EMpVM2vUZ",
        "outputId": "b0663cd9-11ea-49d2-ccf9-ef6d232d6407"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Passing `generation_config` together with generation-related arguments=({'do_sample', 'top_k', 'temperature', 'top_p', 'max_new_tokens'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
            "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì£„ì†¡í•˜ì§€ë§Œ ì œê°€ ì§ì ‘ ì œí’ˆ ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ëŠ” ì—†ì–´ì„œ ë‚´ë…„ë„ ê³µíœ´ì¼ì´ ë‹¤ì´ì–´ë¦¬ì— ì •í™•íˆ í‘œì‹œë˜ì–´ ìˆëŠ”ì§€ ì•Œë ¤ë“œë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê°€ì¥ ì •í™•í•œ ì •ë³´ëŠ” ì œí’ˆ ì„¤ëª…ì„œë‚˜ ì œì¡°ì‚¬ì— ë¬¸ì˜í•˜ì‹œëŠ” ê²ƒì´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì œí’ˆ ë‹´ë‹¹ìê»˜ í™•ì¸í•˜ì‹œë©´ ë¹ ë¥´ê²Œ ë‹µë³€ì„ ë°›ìœ¼ì‹¤ ìˆ˜ ìˆì„ ê±°ì˜ˆìš”!\n"
          ]
        }
      ],
      "source": [
        "output = pipe(messages, max_new_tokens=200, return_full_text=False,\n",
        "              do_sample=True, temperature=0.8, top_k=100, top_p=0.9)\n",
        "print(output[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xg_gz457_mms"
      },
      "source": [
        "## GPT-4oë¡œ ìƒí’ˆ ì§ˆë¬¸ì— ëŒ€í•œ ëŒ€ë‹µ ìƒì„±í•˜ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "NafcUrDw_mU1"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=\"OpenAI í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”\")\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=messages\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEcGMzK5Hjc9",
        "outputId": "6690b962-0a1f-4d8b-e086-2f162a158ba0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì•ˆë…•í•˜ì„¸ìš”! í•´ë‹¹ ë‹¤ì´ì–´ë¦¬ì— ë‚´ë…„ë„ ê³µíœ´ì¼ì´ í‘œì‹œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. ì •í™•í•œ ë‹µë³€ì„ ë“œë¦¬ê¸° ìœ„í•´ ì¡°ê¸ˆ ì‹œê°„ì´ í•„ìš”í•  ê²ƒ ê°™ìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!\n"
          ]
        }
      ],
      "source": [
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4PqAIDiIiN_",
        "outputId": "190152ac-5800-46e0-d583-c137c531db1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì•ˆë…•í•˜ì„¸ìš”! í•´ë‹¹ ë‹¤ì´ì–´ë¦¬ì— ë‚´ë…„ë„ ê³µíœ´ì¼ì´ í‘œì‹œë˜ì–´ ìˆëŠ”ì§€ì— ëŒ€í•œ ì •í™•í•œ ì •ë³´ëŠ” ì œí’ˆ ë‹´ë‹¹ìê°€ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤. ì¡°ê¸ˆë§Œ ê¸°ë‹¤ë ¤ ì£¼ì‹œë©´ ë¹ ë¥¸ ì‹œì¼ ë‚´ì— ë‹µë³€ ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!\n"
          ]
        }
      ],
      "source": [
        "completion = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=messages,\n",
        "    top_p=0.9\n",
        ")\n",
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MoZFdvaZ-36",
        "outputId": "50908eeb-0836-4167-c645-b5baa4c4a319"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì•ˆë…•í•˜ì„¸ìš”! í•´ë‹¹ ë‹¤ì´ì–´ë¦¬ì— ë‚´ë…„ë„ ê³µíœ´ì¼ì´ í‘œì‹œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤. ì •í™•í•œ ë‹µë³€ì„ ìœ„í•´ ë” ì• ìŠ¤ ë¸” ìƒíƒœ ì œí’ˆ ë‹´ë‹¹ìì—ê²Œ ì²´í¬í•´ ë³´ê² ìŠµë‹ˆë‹¤. ì¡°ê¸ˆë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”!\n"
          ]
        }
      ],
      "source": [
        "completion = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=messages,\n",
        "    temperature=1.8\n",
        ")\n",
        "print(completion.choices[0].message.content)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}