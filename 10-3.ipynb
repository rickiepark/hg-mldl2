{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lp7k8Ul6VWg"
      },
      "source": [
        "# ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ë¡œ í…ìŠ¤íŠ¸ ìƒì„±í•˜ê¸°"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekJOhc986a7p"
      },
      "source": [
        "<table align=\"left\"><tr><td>\n",
        "<a href=\"https://colab.research.google.com/github/rickiepark/hg-mldl2/blob/main/10-3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"ì½”ë©ì—ì„œ ì‹¤í–‰í•˜ê¸°\"/></a>\n",
        "</td></tr></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXAONE 3.5 ëª¨ë¸ì€ `transformers` ìµœì‹  ë²„ì „ê³¼ í˜¸í™˜ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì´ ì ˆì˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ê¸° ìœ„í•´ `transformers` 5.1.0 ë²„ì „ì„ ì„¤ì¹˜í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "ZoFfCoeq4ZFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers==5.1.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pU9bLPP7LrC",
        "outputId": "880140d3-5937-4517-f5fa-f4ca5d413dc9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==5.1.0\n",
            "  Downloading transformers-5.1.0-py3-none-any.whl.metadata (31 kB)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from transformers==5.1.0) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==5.1.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==5.1.0) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==5.1.0) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==5.1.0) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers==5.1.0) (0.22.2)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers==5.1.0) (0.24.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers==5.1.0) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==5.1.0) (4.67.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers==5.1.0) (3.24.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers==5.1.0) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers==5.1.0) (1.3.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers==5.1.0) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers==5.1.0) (1.5.4)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers==5.1.0) (4.15.0)\n",
            "Requirement already satisfied: typer>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers==5.1.0) (0.24.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers==5.1.0) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers==5.1.0) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers==5.1.0) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers==5.1.0) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers==5.1.0) (0.16.0)\n",
            "Requirement already satisfied: click>=8.2.1 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->transformers==5.1.0) (8.3.1)\n",
            "Requirement already satisfied: rich>=12.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->transformers==5.1.0) (13.9.4)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->transformers==5.1.0) (0.0.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->transformers==5.1.0) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->transformers==5.1.0) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=12.3.0->typer>=0.24.0->typer-slim->transformers==5.1.0) (0.1.2)\n",
            "Downloading transformers-5.1.0-py3-none-any.whl (10.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 5.0.0\n",
            "    Uninstalling transformers-5.0.0:\n",
            "      Successfully uninstalled transformers-5.0.0\n",
            "Successfully installed transformers-5.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ê¹ƒí—ˆë¸Œì—ì„œ ìœ„ì ¯ ìƒíƒœ ì˜¤ë¥˜ë¥¼ í”¼í•˜ê¸° ìœ„í•´ ì§„í–‰ í‘œì‹œì¤„ì„ ë‚˜íƒ€ë‚´ì§€ ì•Šë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.\n",
        "from transformers.utils import logging\n",
        "\n",
        "logging.disable_progress_bar()"
      ],
      "metadata": {
        "id": "1Q2E2reQUuWB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMzjB7rbojXF"
      },
      "source": [
        "## EXAONE-3.5ë¡œ ìƒí’ˆ ì§ˆë¬¸ì— ëŒ€í•œ ëŒ€ë‹µ ìƒì„±í•˜ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vttIh-QO-1jh",
        "outputId": "49a76fba-082e-4b04-9616-f0f27f18ecde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct:\n",
            "- configuration_exaone.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "A new version of the following files was downloaded from https://huggingface.co/LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct:\n",
            "- modeling_exaone.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(task=\"text-generation\",\n",
        "                model=\"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\",\n",
        "                device=0, trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suKA-HzT4BT3",
        "outputId": "bcdf1338-d2c8-457d-eb1b-88ea45fc37a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Passing `generation_config` together with generation-related arguments=({'max_new_tokens'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
            "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': [{'role': 'system',\n",
              "    'content': 'ë„ˆëŠ” ì‡¼í•‘ëª° í™ˆí˜ì´ì§€ì— ì˜¬ë¼ì˜¨ ì§ˆë¬¸ì— ëŒ€ë‹µí•˜ëŠ” Q&A ì±—ë´‡ì´ì•¼.                  í™•ì •ì ì¸ ë‹µë³€ì„ í•˜ì§€ ë§ê³  ì œí’ˆ ë‹´ë‹¹ìê°€ ì •í™•í•œ ë‹µë³€ì„ í•˜ê¸° ìœ„í•´                  ì‹œê°„ì´ í•„ìš”í•˜ë‹¤ëŠ” ê°„ë‹¨í•˜ê³  ì¹œì ˆí•œ ë‹µë³€ì„ ìƒì„±í•´ì¤˜.'},\n",
              "   {'role': 'user', 'content': 'ì´ ë‹¤ì´ì–´ë¦¬ì— ë‚´ë…„ë„ ê³µíœ´ì¼ì´ í‘œì‹œë˜ì–´ ìˆë‚˜ìš”?'},\n",
              "   {'role': 'assistant',\n",
              "    'content': 'ì•ˆë…•í•˜ì„¸ìš”! ë‹¤ì´ì–´ë¦¬ì— ê´€í•œ ì§ˆë¬¸ ê°ì‚¬í•©ë‹ˆë‹¤. í˜„ì¬ ê³µíœ´ì¼ ì •ë³´ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ê·¸ í•´ì˜ ë‹¬ë ¥ê³¼ ì—°ê´€ë˜ì–´ í‘œì‹œë˜ê³¤ í•©ë‹ˆë‹¤. ì •í™•í•œ ê³µíœ´ì¼ ì •ë³´ëŠ” í•´ë‹¹ ë‹¤ì´ì–´ë¦¬ì˜ ì œì‘ ì—°ë„ì— ë§ì¶°ì ¸ ìˆì„ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤. \\n\\në” ìì„¸í•œ ë‚´ìš©ì„ í™•ì¸í•˜ì‹œê³  ì‹¶ìœ¼ì‹œë‹¤ë©´, ì €í¬ ê³ ê° ì„œë¹„ìŠ¤íŒ€ì— ì—°ë½ì£¼ì‹œê±°ë‚˜, ì§ì ‘ ì €í¬ ì‡¼í•‘ëª°ìœ¼ë¡œ ë°©ë¬¸í•´ ì£¼ì‹œë©´ ë”ìš± ì •í™•í•œ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì‹œê°„ì´ ì¡°ê¸ˆ ê±¸ë¦´ ìˆ˜ ìˆìœ¼ë‹ˆ ì–‘í•´ ë¶€íƒë“œë¦½ë‹ˆë‹¤! ê°ì‚¬í•©ë‹ˆë‹¤.'}]}]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\",\n",
        "     \"content\": \"ë„ˆëŠ” ì‡¼í•‘ëª° í™ˆí˜ì´ì§€ì— ì˜¬ë¼ì˜¨ ì§ˆë¬¸ì— ëŒ€ë‹µí•˜ëŠ” Q&A ì±—ë´‡ì´ì•¼. \\\n",
        "                 í™•ì •ì ì¸ ë‹µë³€ì„ í•˜ì§€ ë§ê³  ì œí’ˆ ë‹´ë‹¹ìê°€ ì •í™•í•œ ë‹µë³€ì„ í•˜ê¸° ìœ„í•´ \\\n",
        "                 ì‹œê°„ì´ í•„ìš”í•˜ë‹¤ëŠ” ê°„ë‹¨í•˜ê³  ì¹œì ˆí•œ ë‹µë³€ì„ ìƒì„±í•´ì¤˜.\"},\n",
        "    {\"role\": \"user\", \"content\": \"ì´ ë‹¤ì´ì–´ë¦¬ì— ë‚´ë…„ë„ ê³µíœ´ì¼ì´ í‘œì‹œë˜ì–´ ìˆë‚˜ìš”?\"}\n",
        "]\n",
        "\n",
        "pipe(messages, max_new_tokens=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6D0ObeXeM8f",
        "outputId": "51b24a66-b21c-4c85-c376-cbf62413f327"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'ë„¤, í•´ë‹¹ ë‹¤ì´ì–´ë¦¬ì— ë‚´ë…„ë„ ê³µíœ´ì¼ì´ ë¯¸ë¦¬ í‘œì‹œë˜ì–´ ìˆëŠ” ê²ƒ ê°™ì•„ìš”! í•˜ì§€ë§Œ ê°€ì¥ ì •í™•í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ë ¤ë©´ ì œí’ˆ ë‹´ë‹¹ìê»˜ ì§ì ‘ ë¬¸ì˜í•˜ì‹œëŠ” ê²Œ ì¢‹ì„ ê²ƒ ê°™ì•„ìš”. ì €í¬ëŠ” ê·¸ ì •ë³´ë¥¼ ì œê³µí•´ë“œë¦´ ìˆ˜ëŠ” ì—†ì§€ë§Œ, ë‹´ë‹¹ìë¶„ê»˜ì„œ ë°”ë¡œ í™•ì¸í•´ ì£¼ì‹¤ ê±°ì˜ˆìš”. ê¶ê¸ˆí•œ ì ì´ ë” ìˆìœ¼ì‹œë©´ ì–¸ì œë“ ì§€ ì•Œë ¤ì£¼ì„¸ìš”! ê°ì‚¬í•©ë‹ˆë‹¤. ğŸ˜Š'}]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "pipe(messages, max_new_tokens=200, return_full_text=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rX3PhJ291jw0",
        "outputId": "451c5cf0-4323-49bd-822e-14c2d1ef64bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Passing `generation_config` together with generation-related arguments=({'do_sample', 'max_new_tokens'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
            "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ë¬¼ë¡ ì´ì£ ! í˜„ì¬ ì €í¬ê°€ í™•ì¸í•  ìˆ˜ ìˆëŠ” ì •ë³´ë¡œëŠ” ë‚´ë…„ì˜ ê³µíœ´ì¼ ì¼ì •ì´ ë‹¤ì´ì–´ë¦¬ í˜ì´ì§€ì— ë¯¸ë¦¬ í‘œì‹œë˜ì–´ ìˆëŠ”ì§€ì— ëŒ€í•œ ì •í™•í•œ ë‹µë³€ì„ ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤. ê³µíœ´ì¼ ì •ë³´ëŠ” ì •ë¶€ì—ì„œ ë°œí‘œí•˜ëŠ” ì¼ì •ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ì—ìš”. ì œí’ˆ ë‹´ë‹¹ìë‹˜ê»˜ì„œ ê°€ì¥ ìµœì‹ ì˜ ì •ë³´ë¥¼ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆë„ë¡ ì¶”ì²œ ë“œë¦´ê²Œìš”. ë‹´ë‹¹ìë‹˜ê»˜ ë¬¸ì˜í•˜ì‹œë©´ ì •í™•í•œ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆì„ ê²ƒ ê°™ì•„ìš”! ê°ì‚¬í•©ë‹ˆë‹¤.\n"
          ]
        }
      ],
      "source": [
        "output = pipe(messages, max_new_tokens=200, return_full_text=False,\n",
        "              do_sample=True)\n",
        "print(output[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pU1i5vWKodxq"
      },
      "source": [
        "## í† í° ë””ì½”ë”© ì „ëµ\n",
        "\n",
        "### ê¸°ë³¸ ìƒ˜í”Œë§"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Eo_cR5cRzZwr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "logits = np.array([1, 2, 3, 4, 100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czQNsdyHyQG4",
        "outputId": "3a77fc4d-da36-4dd1-b4e6-1e106de6b08a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.01122149e-43 2.74878501e-43 7.47197234e-43 2.03109266e-42\n",
            " 1.00000000e+00]\n"
          ]
        }
      ],
      "source": [
        "from scipy.special import softmax\n",
        "\n",
        "probas = softmax(logits)\n",
        "print(probas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzV5-Erxzb3T",
        "outputId": "80d069ea-36d2-48bf-ecf7-4f8443265ba4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  0,   0,   0,   0, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "np.random.multinomial(100, probas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1hiMBbP2fo-",
        "outputId": "470bd000-fdaf-4a5d-ac1c-8e970af1c3cd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([11, 18, 16, 17, 38])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "probas = softmax(logits/100)\n",
        "np.random.multinomial(100, probas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVL3CbAW2ki5",
        "outputId": "07da1d72-c51e-4791-a024-a5053fb4d0cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Passing `generation_config` together with generation-related arguments=({'temperature', 'do_sample', 'max_new_tokens'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
            "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Hello ê³ ê°SirOr Madam,, í˜„ì¬ë¡œë¹„ì¹˜ì‹œìŠ¤í…œì˜¤ëŸ¬ë””ì–´ë¦¬ì–¼ ì—…ë°ì´íŠ¸ì‚¬í•­ì˜¤í‘œì‹œê°€ì˜¤ëŠ”ë° ì‹œê°„ì´ì§€ë‚˜ë‹¤ë‚˜ ë‚´ë…„ holidayendar ë¶€ë¶„ì˜ìì„¸ì„í™•ìƒí•˜ëŸ¬ ì „ë¬¸ê°€ì´ë¶„ì˜ ì•ˆë‚´ì£¼ì‹œë”êµ°ìš”.!ì¢€ í›„ë¡œ ì €í¬ íŒ€ì›ì—ì„œ í™•ì •ì‚¬í•­ì „í• ì˜ˆì •ì´ë‹¤ë¯€ë¡œ ê·¸ë•Œ ì •í™•íˆì•Œë ¤ì£¼ì§€ ëª»í•  ë“¯í•œ ë¶€ë¶„ì—ëŒ€í•¨ì€ ì‚¬ê³¼ë“œ ë¦´ë˜ìš” ~. ì‹œê°„ ì—¬ìœ í•˜ì‹œë ¤ í•˜ì…¨ë˜ ëª©ì ë“¤ê¹Œì§€ì˜í•˜ì‚¬ ê¸°ëŒ€ ë§ì´ í•´ìš”! ê¶ê¸ˆí•˜ë‹¤ë©° ë¹ ë¥¸ ì¡°íšŒë¡œ ë” ìœ ìš©í•œë•Œë§ë“œë¦¬ëŠ ë° ë•í˜€ ë“œë ¤í–ˆìŒ ì¢‹ì•˜ì–´í• í„°êµ¬ìš” â™¡ â˜º!\n"
          ]
        }
      ],
      "source": [
        "output = pipe(messages, max_new_tokens=200, return_full_text=False,\n",
        "              do_sample=True, temperature=10.0)\n",
        "print(output[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOp0DaJq5lTc",
        "outputId": "bf5f7cbe-b620-49e4-f737-df126a98d631"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì•ˆë…•í•˜ì„¸ìš”! ë‹¤ì´ì–´ë¦¬ì— ë‚´ë…„ì˜ ê³µíœ´ì¼ì´ ë¯¸ë¦¬ í‘œì‹œë˜ì–´ ìˆëŠ”ì§€ì— ëŒ€í•´ ì •í™•í•œ ë‹µë³€ì„ ë“œë¦¬ê¸° ìœ„í•´ì„œëŠ” ì œí’ˆ ë‹´ë‹¹ìì—ê²Œ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤. í˜„ì¬ë¡œì„  ì§ì ‘ í™•ì¸ì´ ì–´ë ¤ìš°ë‹ˆ, ì €í¬ê°€ ì•ˆë‚´ë“œë¦´ ìˆ˜ ìˆëŠ” ë°©ë²•ìœ¼ë¡œëŠ” ê³ ê°ì„¼í„°ì— ì—°ë½í•˜ì‹œê±°ë‚˜, ì œí’ˆ í˜ì´ì§€ ë‚´ì˜ ë¬¸ì˜ ê²Œì‹œíŒì„ í†µí•´ ì§ˆë¬¸í•´ ë³´ì‹œëŠ” ê²ƒì´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ë‹´ë‹¹ìë¶„ê»˜ì„œ ë¹ ë¥´ê²Œ ë‹µë³€í•´ ì£¼ì‹¤ ê±°ì˜ˆìš”! ê°ì‚¬í•©ë‹ˆë‹¤.\n"
          ]
        }
      ],
      "source": [
        "output = pipe(messages, max_new_tokens=200, return_full_text=False,\n",
        "              do_sample=True, temperature=0.001)\n",
        "print(output[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRH9m1tAox0c"
      },
      "source": [
        "### top-k ìƒ˜í”Œë§"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0U02SYXo0tA",
        "outputId": "d5378a5a-e027-454a-b06b-ca0d729d71a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Passing `generation_config` together with generation-related arguments=({'do_sample', 'max_new_tokens', 'top_k'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
            "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ë„¤, ë§ìŠµë‹ˆë‹¤! ë‹¤ì´ì–´ë¦¬ì— ë‚´ë…„ë„ ê³µíœ´ì¼ë“¤ì´ ì˜ í‘œì‹œë˜ì–´ ìˆì„ ê±°ì˜ˆìš”. í•˜ì§€ë§Œ ì •í™•í•œ ê³µíœ´ì¼ ë‚´ìš©ì€ ë§¤ë…„ ì¡°ê¸ˆì”© ë³€ë™ì´ ìˆì„ ìˆ˜ ìˆìœ¼ë‹ˆ, ê°€ì¥ í™•ì‹¤í•œ ì •ë³´ë¥¼ ì›í•˜ì‹ ë‹¤ë©´ ì €í¬ ì›¹ì‚¬ì´íŠ¸ì˜ ê³ ê°ì„¼í„°ë‚˜ ê´€ë ¨ ê³µíœ´ì¼ ì•ˆë‚´ í˜ì´ì§€ë¥¼ í™•ì¸í•´ ë³´ì‹œëŠ” ê²Œ ì¢‹ì„ ê²ƒ ê°™ì•„ìš”. ê±°ê¸°ì„œ ê°€ì¥ ìµœì‹ ì˜ ì •ë³´ë¥¼ ì–»ìœ¼ì‹¤ ìˆ˜ ìˆì„ ê±°ì˜ˆìš”! ê°ì‚¬í•©ë‹ˆë‹¤.\n"
          ]
        }
      ],
      "source": [
        "output = pipe(messages, max_new_tokens=200, return_full_text=False,\n",
        "              do_sample=True, top_k=10)\n",
        "print(output[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sd8Bumm5qcKs",
        "outputId": "f9f5fd27-7206-4f3a-b2fd-bce4dc0feae0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Passing `generation_config` together with generation-related arguments=({'temperature', 'do_sample', 'max_new_tokens', 'top_k'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
            "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ë„¤ê°€ ì›í•˜ëŠ” íŠ¹ì • ê³µíœ´ì¼ ë‚ ì§œë“¤ì´ ë‚´ë…„ì— ì–´ë–»ê²Œ Diary ë‚´ë¶€ ë˜ëŠ” ì›¹ ìƒì—ë„ ì •í™•íˆ ë°˜ì˜ë˜ë„ë¡ ì—…ë°ì´íŠ¸í•˜ëŠì§€ ì •í™•íˆëŠ” ì €í•œí…Œë‚˜ ì‡¼í•‘ëª° ë‚´ë¶€ì˜ ì œí’ˆ ê°œë°œì§„ê»˜ í™•ì¸ì„ í•´ ë´ì•¼ í•˜ëŠ”ë° í˜„ì¬ ì œ ëŠ¥ë ¥ ë‚´ì˜ ë‹µë³€ ì‹œê°„ì€ í•œì •ìƒ ìì„¸í•˜ì‹  ì§ˆë¬¸ì— ì •í™•íˆ ëŒ€ë‹µí•´ ë“œê¸° ì–´ë ¤ì›Œ unfortunately ğŸ˜¥ í•˜ì§€ë§Œ ê³ ê°ì„œë¹„ìŠ¤ íŒ€ì´ë‚˜ ê´€ë ¨ ìƒí’ˆì„ ì±…ì„ì§€ê³  ê³„ì‹  ê´€ë¦¬ìê»˜ ì¦‰ì‹œ ë¬¸ì˜í•´ ë³´ì‹œê³  ì‹ ì†í•  ìˆ˜ë¡ ë„ì›€ì´ ë˜ê² êµ¬ë‚˜ í•˜ì‹œì£ ~ ê³ ê°ì„œë¹„ìŠ¤ë¡œ ë¬¸ì˜ë¥¼ ë‚¨ê¸°ì‹  ì ìœ¼ë¡œ ìƒê°í•˜ë©´ ë”ìš± êµ¬ì²´ë‹µ ë“œë¦¬ë ¤ Effortsë¥¼ í•˜ë ¤ê³  í•´ìš” ^_^ regards!\n"
          ]
        }
      ],
      "source": [
        "output = pipe(messages, max_new_tokens=200, return_full_text=False,\n",
        "              do_sample=True, top_k=10, temperature=10.0)\n",
        "print(output[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEfCc4wCrcXd"
      },
      "source": [
        "### top-p ìƒ˜í”Œë§"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7sZieDNreBn",
        "outputId": "937f740f-a375-4c5c-fb5b-9c00a934e133"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Passing `generation_config` together with generation-related arguments=({'do_sample', 'top_p', 'max_new_tokens'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
            "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì•ˆë…•í•˜ì„¸ìš”! ë‹¤ì´ì–´ë¦¬ì˜ ê³µíœ´ì¼ ì •ë³´ì— ëŒ€í•´ ê¶ê¸ˆí•˜ì‹œêµ°ìš”! ì •í™•í•œ ë‹µë³€ì„ ë“œë¦¬ê¸° ìœ„í•´ì„œëŠ” ì œí’ˆ ë‹´ë‹¹ìë‹˜ê»˜ì„œ ì§ì ‘ í™•ì¸í•˜ì‹¤ í•„ìš”ê°€ ìˆì–´ìš”. í˜„ì¬ë¡œì„  í™•ì‹¤í•œ ë‹µë³€ì„ ë“œë¦¬ê¸° ì–´ë µì§€ë§Œ, ì œí’ˆ ì •ë³´ í˜ì´ì§€ë‚˜ ê³ ê°ì„¼í„°ë¡œ ë¬¸ì˜í•˜ì‹œë©´ ê°€ì¥ ë¹ ë¥´ê³  ì •í™•í•œ ë‹µë³€ì„ ë°›ìœ¼ì‹¤ ìˆ˜ ìˆì„ ê±°ì˜ˆìš”. ê¶ê¸ˆí•œ ì ì´ ë” ìˆìœ¼ì‹œë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”!\n"
          ]
        }
      ],
      "source": [
        "output = pipe(messages, max_new_tokens=200, return_full_text=False,\n",
        "              do_sample=True, top_p=0.9)\n",
        "print(output[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QN0EMpVM2vUZ",
        "outputId": "239f2c1d-e91d-479b-ab2f-2fcf6de02054"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Passing `generation_config` together with generation-related arguments=({'top_p', 'temperature', 'do_sample', 'max_new_tokens', 'top_k'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
            "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì•ˆë…•í•˜ì„¸ìš”! ë‹¤ì´ì–´ë¦¬ì— ë‚´ë…„ë„ ê³µíœ´ì¼ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ì— ëŒ€í•´ ì •í™•íˆ ë§ì”€ë“œë¦¬ê¸° ìœ„í•´ì„œëŠ” ì œí’ˆ ë‹´ë‹¹ìì—ê²Œ í™•ì¸í•´ì•¼ í•  ê²ƒ ê°™ì•„ìš”. í˜„ì¬ëŠ” ì¤€ë¹„ ì¤‘ì¸ ì •ë³´ê°€ í•„ìš”í•œë°, ì‹œê°„ì´ ì¡°ê¸ˆ ë” ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹´ë‹¹ìê»˜ ì§ì ‘ ë¬¸ì˜í•˜ì‹œëŠ” ê²Œ ê°€ì¥ í™•ì‹¤í•œ ë°©ë²•ì¼ ê²ƒ ê°™ì•„ìš”! ë¹ ë¥´ê²Œ ë‹µë³€ë“œë¦¬ì§€ ëª»í•´ ì£„ì†¡í•©ë‹ˆë‹¤. ê¶ê¸ˆí•œ ì ì´ ë” ìˆìœ¼ì‹œë©´ ì–¸ì œë“ ì§€ ì•Œë ¤ì£¼ì„¸ìš”.\n"
          ]
        }
      ],
      "source": [
        "output = pipe(messages, max_new_tokens=200, return_full_text=False,\n",
        "              do_sample=True, temperature=0.8, top_k=100, top_p=0.9)\n",
        "print(output[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xg_gz457_mms"
      },
      "source": [
        "## GPT-4oë¡œ ìƒí’ˆ ì§ˆë¬¸ì— ëŒ€í•œ ëŒ€ë‹µ ìƒì„±í•˜ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "NafcUrDw_mU1"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=\"OpenAI í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”\")\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=messages\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEcGMzK5Hjc9",
        "outputId": "57579d6d-c4cd-4abb-c4bf-25b53c69c0ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì•ˆë…•í•˜ì„¸ìš”! í•´ë‹¹ ë‹¤ì´ì–´ë¦¬ì— ë‚´ë…„ë„ ê³µíœ´ì¼ì´ í‘œì‹œë˜ì–´ ìˆëŠ”ì§€ì— ëŒ€í•œ ì •í™•í•œ ì •ë³´ëŠ” ì œí’ˆ ë‹´ë‹¹ìê°€ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤. ì¡°ê¸ˆë§Œ ê¸°ë‹¤ë ¤ ì£¼ì‹œë©´ ì •í™•í•œ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!\n"
          ]
        }
      ],
      "source": [
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4PqAIDiIiN_",
        "outputId": "c1718597-85f9-49a3-d9c9-7e1c89f719b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì•ˆë…•í•˜ì„¸ìš”! ë¬¸ì˜í•´ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤. í•´ë‹¹ ë‹¤ì´ì–´ë¦¬ì— ë‚´ë…„ë„ ê³µíœ´ì¼ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. ì¡°ê¸ˆë§Œ ì‹œê°„ì„ ì£¼ì‹œë©´ ì œí’ˆ ë‹´ë‹¹ìê°€ ì •í™•í•œ ì •ë³´ë¥¼ ì•Œë ¤ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!\n"
          ]
        }
      ],
      "source": [
        "completion = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=messages,\n",
        "    top_p=0.9\n",
        ")\n",
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MoZFdvaZ-36",
        "outputId": "9f64c7d3-6fe3-43d4-a39d-5c6e90ecb430"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì´ ì•ˆì— ê³µíœ´ì¼ í‘œì‹œ ê´€ë ¨ ì •ë³´ê°€ ìˆëŠ”ì§€ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤. ì¢€ ë” ì •í™•í•œ ë‹µë³€ì„ ìœ„í•´ ì œí’ˆ ë‹´ë‹¹ìì—ê²Œ í™•ì¸ì´ í•„ìš”í•˜ë¯€ë¡œ ì ì‹œ ì‹œê°„ì´ ê±¸ë¦¬ê² ìŠµë‹ˆë‹¤. ì˜ê²¬ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤!\n"
          ]
        }
      ],
      "source": [
        "completion = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=messages,\n",
        "    temperature=1.8\n",
        ")\n",
        "print(completion.choices[0].message.content)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}